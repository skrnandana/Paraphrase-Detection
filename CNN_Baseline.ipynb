{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries**"
      ],
      "metadata": {
        "id": "DDukk_rkKie2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n"
      ],
      "metadata": {
        "id": "3YyeFzUAKivN"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the Dataset**"
      ],
      "metadata": {
        "id": "5oFPT8lnKpNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "train_df = pd.read_csv('/content/labeled_final_train.csv')\n",
        "val_df = pd.read_csv('/content/labeled_final_validation.csv')\n",
        "test_df = pd.read_csv('/content/labeled_final_test.csv')"
      ],
      "metadata": {
        "id": "LZKgGSa38i4U"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare the Dataset**"
      ],
      "metadata": {
        "id": "zNAIYtIXK2lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the sentence pairs\n",
        "\n",
        "train_texts = train_df['sentence1'] + \" [SEP] \" + train_df['sentence2']\n",
        "val_texts = val_df['sentence1'] + \" [SEP] \" + val_df['sentence2']\n",
        "test_texts = test_df['sentence1'] + \" [SEP] \" + test_df['sentence2']\n",
        "\n",
        "all_texts = pd.concat([train_texts, val_texts, test_texts])\n"
      ],
      "metadata": {
        "id": "sihX9y7DK4vh"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "K5mwDrhkNXt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize texts\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(all_texts)\n",
        "\n",
        "# Convert the texts in the train, validation, and test sets\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n"
      ],
      "metadata": {
        "id": "PYEQ1z2xK9fi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sequence Padding**"
      ],
      "metadata": {
        "id": "CUsvgALkNbW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in val_sequences), max(len(seq) for seq in test_sequences))\n",
        "\n",
        "# Pad the sequences so that they all have the same length.\n",
        "train_data = pad_sequences(train_sequences, maxlen=max_length)\n",
        "val_data = pad_sequences(val_sequences, maxlen=max_length)\n",
        "test_data = pad_sequences(test_sequences, maxlen=max_length)"
      ],
      "metadata": {
        "id": "0FkaOMbxNc1d"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Label Preparation**"
      ],
      "metadata": {
        "id": "9E2DFZecNenc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract labels from the dataframes\n",
        "train_labels = train_df['label'].values\n",
        "val_labels = val_df['label'].values\n",
        "test_labels = test_df['label'].values"
      ],
      "metadata": {
        "id": "C0kirN9mNfC5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining the CNN Model**"
      ],
      "metadata": {
        "id": "blcq_eePNSWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length),  # Increased embedding dimensions\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    Dropout(0.5),  # Dropout layer for regularization\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu'),  # Additional Conv1D layer\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(10, activation='relu'),\n",
        "    Dropout(0.5),  # Another dropout layer\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ0fr8ic846u",
        "outputId": "32bc3bd0-786e-4fed-a2de-2f011cdea408"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 63, 100)           3607700   \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 59, 128)           64128     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 59, 128)           0         \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 57, 64)            24640     \n",
            "                                                                 \n",
            " global_max_pooling1d_3 (Gl  (None, 64)                0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 10)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3697129 (14.10 MB)\n",
            "Trainable params: 3697129 (14.10 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and Evaluate the Model**"
      ],
      "metadata": {
        "id": "Om-YlO46N1x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(train_data, train_labels, batch_size=32, epochs=3, validation_data=(val_data, val_labels))\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_acc = model.evaluate(val_data, val_labels)\n",
        "print(f'Validation Accuracy: {val_acc}')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
        "print(f'Test Accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqfgFufT88PV",
        "outputId": "f86ad956-760f-444b-c882-5370a00515fc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1544/1544 [==============================] - 162s 104ms/step - loss: 0.6873 - accuracy: 0.5576 - val_loss: 0.6875 - val_accuracy: 0.5615\n",
            "Epoch 2/3\n",
            "1544/1544 [==============================] - 159s 103ms/step - loss: 0.6803 - accuracy: 0.5699 - val_loss: 0.6849 - val_accuracy: 0.5606\n",
            "Epoch 3/3\n",
            "1544/1544 [==============================] - 158s 102ms/step - loss: 0.6666 - accuracy: 0.5887 - val_loss: 0.6857 - val_accuracy: 0.5616\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6857 - accuracy: 0.5616\n",
            "Validation Accuracy: 0.5616250038146973\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.6830 - accuracy: 0.5651\n",
            "Test Accuracy: 0.5651249885559082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicting Paraphrase**"
      ],
      "metadata": {
        "id": "j8lSxGvgONL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_paraphrase(sentence1, sentence2):\n",
        "    combined_text = sentence1 + \" [SEP] \" + sentence2\n",
        "    sequence = tokenizer.texts_to_sequences([combined_text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_length)\n",
        "    prediction = model.predict(padded_sequence)\n",
        "    return \"Paraphrase\" if prediction[0][0] > 0.5 else \"Not Paraphrase\"\n"
      ],
      "metadata": {
        "id": "wQJistKiCar9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
        "sentence2 = \"A fast, dark-colored fox leaps above a slow-moving dog.\"\n",
        "print(predict_paraphrase(sentence1, sentence2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_st92Kk8Ckkl",
        "outputId": "d5d83c68-bcc6-4fb7-aa98-89d9c4cd9ada"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 77ms/step\n",
            "Not Paraphrase\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence3 = \"My name is Sam and I am a good boy.\"\n",
        "sentence4 = \"I'm Sam and I consider myself to be a well-behaved young man..\"\n",
        "print(predict_paraphrase(sentence3, sentence4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMlOzUXtCvpp",
        "outputId": "666808b0-a8ad-4c42-ed87-a4f6831145d2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "Not Paraphrase\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence5 = \"BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with two objectives:\"\n",
        "sentence6 = \"BERT is a transformer-based model that was pre-trained using a vast collection of English text in a self-supervised manner. This entails that it underwent pre-training solely on raw texts without any human-annotated labels, allowing it to leverage a vast array of publicly accessible data through an automated process that creates inputs and labels directly from the texts. Specifically, it was designed with two primary pre-training objectives.\"\n",
        "print(predict_paraphrase(sentence5, sentence6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-iqLQWWCw5A",
        "outputId": "b7fbe43b-af4c-4813-97ed-c09c70ba83fc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n",
            "Not Paraphrase\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence7 = \"The conclusion that explicit part descriptions enhance abstract reasoning in both humans and models is well-supported by the data.\"\n",
        "sentence8 = \"The data strongly supports the idea that detailed descriptions of parts improve abstract reasoning in both humans and computational models.\"\n",
        "print(predict_paraphrase(sentence7, sentence8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EyYg2ZWC5Te",
        "outputId": "f31121dc-a82c-4a77-d255-ea15dc47e7e2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            "Not Paraphrase\n"
          ]
        }
      ]
    }
  ]
}